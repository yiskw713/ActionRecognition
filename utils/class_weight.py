import torch


def get_class_weight():
    """
    Class weight for CrossEntropy in Kinetics
    Class weight is calculated in the way described in:
        D. Eigen and R. Fergus, “Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture,” in ICCV,
        openaccess: https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Eigen_Predicting_Depth_Surface_ICCV_2015_paper.pdf

    Class IDs can be obtained from `utils/class_label_map.py`

    if you calculate your dataset, please try this code:
    ```
    df = pd.read_csv(CSV_PATH)

    nums = {}
    for i in range(400):
        nums[i] = 0;

    for i in range(len(df)):
        nums[df.iloc[i, 2]] += 1

    class_num = []
    for val in nums.values():
        class_num.append(val)

    class_num = torch.tensor(class_num)

    total = class_num.sum().item()

    frequency = class_num.float() / total
    median = torch.median(frequency)

    class_weight = median / frequency

    ```
    """

    class_weight = torch.tensor([
        0.6024, 0.6104, 1.8293, 2.2989, 1.8293, 0.6018, 0.6154, 1.3857, 1.5228,
        1.8293, 1.2987, 0.7712, 0.8850, 1.4320, 0.6522, 1.3304, 0.7566, 2.1429,
        0.6192, 0.6276, 1.2371, 2.3077, 0.6652, 1.0638, 0.6030, 2.3529, 1.3423,
        0.6000, 1.3187, 0.7853, 1.1111, 0.6445, 0.9494, 1.9737, 0.7519, 1.5584,
        0.7643, 0.6006, 2.1352, 2.1583, 0.6615, 0.8559, 0.6018, 0.6369, 1.4706,
        1.2848, 1.0657, 1.1516, 0.9901, 0.6593, 0.8671, 0.9983, 1.0638, 1.9293,
        1.4815, 0.6030, 0.7823, 1.7493, 1.6529, 0.7968, 0.8287, 1.3393, 2.0202,
        1.0714, 1.4085, 1.1009, 2.2814, 1.1719, 0.6186, 0.6079, 0.7059, 1.2793,
        2.3622, 1.8868, 1.1450, 0.6936, 2.0000, 0.6000, 0.7491, 0.6742, 0.8511,
        1.4528, 1.0676, 0.9709, 0.6036, 1.0471, 0.8696, 0.7417, 0.9146, 1.2959,
        2.3529, 1.1494, 1.4458, 0.6479, 1.3483, 1.9231, 1.9293, 0.7500, 2.0270,
        0.7762, 1.3304, 1.4052, 2.3529, 0.6179, 0.7772, 1.0563, 2.3166, 0.6270,
        0.6445, 0.8380, 1.7442, 1.6393, 1.0000, 1.5831, 1.4286, 0.7722, 0.6024,
        1.5000, 1.7143, 2.2472, 2.0690, 1.3274, 2.0619, 0.6000, 0.7282, 0.6842,
        0.6390, 0.8889, 1.1385, 1.0490, 0.6565, 1.1009, 0.8264, 0.7595, 0.7389,
        1.3100, 2.0548, 2.1352, 1.1811, 1.0169, 0.7472, 1.0929, 0.8746, 0.6438,
        2.2642, 1.2959, 1.2097, 0.6036, 0.6012, 0.6369, 1.2220, 0.7444, 0.8889,
        0.6515, 1.8868, 2.1429, 1.0417, 1.4493, 1.6304, 0.6116, 1.2712, 0.8734,
        0.8633, 1.4815, 0.6048, 1.5504, 0.7874, 0.6061, 2.2305, 0.7762, 1.1583,
        1.7964, 0.6104, 1.1696, 0.8785, 1.5228, 1.0292, 0.9288, 1.1091, 1.1788,
        0.7732, 2.1277, 0.8798, 0.9852, 1.9169, 2.0690, 1.1342, 1.1788, 0.6018,
        0.9868, 2.1127, 2.1739, 0.6018, 0.6186, 1.8293, 1.3575, 1.1494, 0.7229,
        1.3158, 0.6042, 2.1739, 0.6018, 2.2222, 1.0309, 0.8380, 0.9231, 0.9788,
        1.6807, 0.8415, 0.6704, 1.3575, 1.9544, 0.9868, 0.9901, 0.9317, 1.4742,
        2.1583, 0.7732, 0.7547, 0.8721, 0.6030, 0.6085, 1.0221, 0.6438, 0.8571,
        0.6881, 1.6043, 0.7509, 1.2346, 0.9419, 0.7916, 1.8462, 0.6091, 0.7009,
        0.6006, 0.7802, 1.0582, 1.8868, 1.0309, 1.1494, 0.6061, 1.1070, 0.6091,
        0.6006, 0.7823, 0.7229, 0.6036, 0.6006, 0.7143, 0.6018, 0.6048, 0.9160,
        1.0067, 0.7186, 0.6667, 0.6179, 0.6985, 1.5152, 0.6000, 1.7647, 1.2903,
        0.6529, 0.6000, 1.9048, 0.6006, 2.1898, 2.2642, 1.8293, 1.0601, 0.6289,
        1.0929, 1.7341, 1.8405, 0.6091, 1.1429, 0.8380, 1.3158, 0.8065, 0.6036,
        2.1898, 0.7407, 2.1505, 0.8357, 0.6006, 1.4151, 0.8996, 0.7335, 1.8237,
        1.2220, 0.8163, 2.1898, 0.9950, 0.7299, 1.6667, 0.7160, 1.2903, 1.3483,
        2.0339, 0.7168, 0.8230, 2.3715, 0.8850, 0.7134, 1.9737, 0.6018, 0.8996,
        0.6067, 0.6659, 0.6054, 1.8349, 1.5385, 1.7751, 1.6901, 0.9360, 1.8927,
        0.9600, 0.6237, 0.8463, 0.7566, 1.6901, 2.3904, 0.6961, 0.7595, 0.6024,
        1.3216, 0.7117, 0.6085, 0.7905, 1.8750, 2.3346, 0.6006, 0.9662, 2.0408,
        1.0563, 0.8837, 1.8634, 0.8242, 0.9934, 1.3216, 0.6472, 0.8785, 1.1364,
        1.6529, 2.3166, 1.8018, 1.8576, 0.6522, 2.6316, 0.6224, 0.7500, 0.9023,
        1.0850, 1.3699, 1.2959, 1.7291, 1.0791, 0.9009, 1.2397, 0.6283, 1.3043,
        0.6018, 1.9231, 1.9108, 1.8072, 0.9434, 0.7212, 1.1650, 0.9449, 2.5316,
        0.8633, 1.1472, 0.8475, 2.3438, 0.7614, 1.4963, 2.5210, 1.4563, 2.1429,
        0.6030, 0.6682, 0.8415, 2.1661, 0.7833, 0.9756, 2.2222, 1.1299, 1.5504,
        0.9836, 1.0508, 0.7519, 1.0118, 0.9852, 2.2556, 0.6224, 0.8439, 1.7699,
        1.0256, 2.4096, 0.6042, 0.6303
    ])

    return class_weight
