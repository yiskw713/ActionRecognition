import torch


def get_class_weight(n_classes=400):
    """
    Class weight for CrossEntropy in Kinetics
    Class weight is calculated in the way described in:
        D. Eigen and R. Fergus, “Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture,” in ICCV,
        openaccess: https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Eigen_Predicting_Depth_Surface_ICCV_2015_paper.pdf

    Class IDs can be obtained from `utils/class_label_map.py`

    if you calculate your dataset, please try this code:
    ```
    df = pd.read_csv(CSV_PATH)

    nums = {}
    for i in range(400):
        nums[i] = 0;

    for i in range(len(df)):
        nums[df.iloc[i, 1]] += 1

    class_num = []
    for val in nums.values():
        class_num.append(val)

    class_num = torch.tensor(class_num)

    total = class_num.sum().item()

    frequency = class_num.float() / total
    median = torch.median(frequency)

    class_weight = median / frequency

    ```
    """
    if n_classes == 400:
        class_weight = torch.tensor([
            0.6024, 0.6104, 1.8293, 2.2989, 1.8293, 0.6018, 0.6154, 1.3857, 1.5228,
            1.8293, 1.2987, 0.7712, 0.8850, 1.4320, 0.6522, 1.3304, 0.7566, 2.1429,
            0.6192, 0.6276, 1.2371, 2.3077, 0.6652, 1.0638, 0.6030, 2.3529, 1.3423,
            0.6000, 1.3187, 0.7853, 1.1111, 0.6445, 0.9494, 1.9737, 0.7519, 1.5584,
            0.7643, 0.6006, 2.1352, 2.1583, 0.6615, 0.8559, 0.6018, 0.6369, 1.4706,
            1.2848, 1.0657, 1.1516, 0.9901, 0.6593, 0.8671, 0.9983, 1.0638, 1.9293,
            1.4815, 0.6030, 0.7823, 1.7493, 1.6529, 0.7968, 0.8287, 1.3393, 2.0202,
            1.0714, 1.4085, 1.1009, 2.2814, 1.1719, 0.6186, 0.6079, 0.7059, 1.2793,
            2.3622, 1.8868, 1.1450, 0.6936, 2.0000, 0.6000, 0.7491, 0.6742, 0.8511,
            1.4528, 1.0676, 0.9709, 0.6036, 1.0471, 0.8696, 0.7417, 0.9146, 1.2959,
            2.3529, 1.1494, 1.4458, 0.6479, 1.3483, 1.9231, 1.9293, 0.7500, 2.0270,
            0.7762, 1.3304, 1.4052, 2.3529, 0.6179, 0.7772, 1.0563, 2.3166, 0.6270,
            0.6445, 0.8380, 1.7442, 1.6393, 1.0000, 1.5831, 1.4286, 0.7722, 0.6024,
            1.5000, 1.7143, 2.2472, 2.0690, 1.3274, 2.0619, 0.6000, 0.7282, 0.6842,
            0.6390, 0.8889, 1.1385, 1.0490, 0.6565, 1.1009, 0.8264, 0.7595, 0.7389,
            1.3100, 2.0548, 2.1352, 1.1811, 1.0169, 0.7472, 1.0929, 0.8746, 0.6438,
            2.2642, 1.2959, 1.2097, 0.6036, 0.6012, 0.6369, 1.2220, 0.7444, 0.8889,
            0.6515, 1.8868, 2.1429, 1.0417, 1.4493, 1.6304, 0.6116, 1.2712, 0.8734,
            0.8633, 1.4815, 0.6048, 1.5504, 0.7874, 0.6061, 2.2305, 0.7762, 1.1583,
            1.7964, 0.6104, 1.1696, 0.8785, 1.5228, 1.0292, 0.9288, 1.1091, 1.1788,
            0.7732, 2.1277, 0.8798, 0.9852, 1.9169, 2.0690, 1.1342, 1.1788, 0.6018,
            0.9868, 2.1127, 2.1739, 0.6018, 0.6186, 1.8293, 1.3575, 1.1494, 0.7229,
            1.3158, 0.6042, 2.1739, 0.6018, 2.2222, 1.0309, 0.8380, 0.9231, 0.9788,
            1.6807, 0.8415, 0.6704, 1.3575, 1.9544, 0.9868, 0.9901, 0.9317, 1.4742,
            2.1583, 0.7732, 0.7547, 0.8721, 0.6030, 0.6085, 1.0221, 0.6438, 0.8571,
            0.6881, 1.6043, 0.7509, 1.2346, 0.9419, 0.7916, 1.8462, 0.6091, 0.7009,
            0.6006, 0.7802, 1.0582, 1.8868, 1.0309, 1.1494, 0.6061, 1.1070, 0.6091,
            0.6006, 0.7823, 0.7229, 0.6036, 0.6006, 0.7143, 0.6018, 0.6048, 0.9160,
            1.0067, 0.7186, 0.6667, 0.6179, 0.6985, 1.5152, 0.6000, 1.7647, 1.2903,
            0.6529, 0.6000, 1.9048, 0.6006, 2.1898, 2.2642, 1.8293, 1.0601, 0.6289,
            1.0929, 1.7341, 1.8405, 0.6091, 1.1429, 0.8380, 1.3158, 0.8065, 0.6036,
            2.1898, 0.7407, 2.1505, 0.8357, 0.6006, 1.4151, 0.8996, 0.7335, 1.8237,
            1.2220, 0.8163, 2.1898, 0.9950, 0.7299, 1.6667, 0.7160, 1.2903, 1.3483,
            2.0339, 0.7168, 0.8230, 2.3715, 0.8850, 0.7134, 1.9737, 0.6018, 0.8996,
            0.6067, 0.6659, 0.6054, 1.8349, 1.5385, 1.7751, 1.6901, 0.9360, 1.8927,
            0.9600, 0.6237, 0.8463, 0.7566, 1.6901, 2.3904, 0.6961, 0.7595, 0.6024,
            1.3216, 0.7117, 0.6085, 0.7905, 1.8750, 2.3346, 0.6006, 0.9662, 2.0408,
            1.0563, 0.8837, 1.8634, 0.8242, 0.9934, 1.3216, 0.6472, 0.8785, 1.1364,
            1.6529, 2.3166, 1.8018, 1.8576, 0.6522, 2.6316, 0.6224, 0.7500, 0.9023,
            1.0850, 1.3699, 1.2959, 1.7291, 1.0791, 0.9009, 1.2397, 0.6283, 1.3043,
            0.6018, 1.9231, 1.9108, 1.8072, 0.9434, 0.7212, 1.1650, 0.9449, 2.5316,
            0.8633, 1.1472, 0.8475, 2.3438, 0.7614, 1.4963, 2.5210, 1.4563, 2.1429,
            0.6030, 0.6682, 0.8415, 2.1661, 0.7833, 0.9756, 2.2222, 1.1299, 1.5504,
            0.9836, 1.0508, 0.7519, 1.0118, 0.9852, 2.2556, 0.6224, 0.8439, 1.7699,
            1.0256, 2.4096, 0.6042, 0.6303
        ])
    elif n_classes == 700:
        class_weight = torch.tensor([
            0.7696, 0.9452, 1.4500, 0.9108, 1.1905, 1.3810, 1.2247, 1.3352, 1.0985,
            0.7608, 1.1417, 0.8885, 0.8333, 1.1924, 1.4300, 0.7713, 1.2205, 0.9477,
            1.5969, 1.2787, 0.8333, 0.7915, 0.8831, 0.7898, 1.4133, 0.9745, 0.7529,
            0.8519, 0.7560, 1.1808, 1.4796, 0.8918, 0.8353, 1.5041, 0.9904, 0.7680,
            0.8101, 1.6111, 0.9189, 1.0000, 0.8362, 1.0126, 1.0805, 0.8029, 1.1694,
            0.8314, 0.7413, 1.1240, 1.3451, 0.9615, 0.8029, 0.8211, 0.8183, 1.0447,
            0.8092, 0.9247, 1.3451, 1.1346, 0.7390, 1.5231, 0.7898, 0.7616, 1.0197,
            1.0372, 0.9119, 1.2565, 1.1508, 0.8470, 1.1188, 0.7497, 1.4300, 0.9680,
            0.7505, 0.7829, 1.3679, 1.5625, 1.2877, 1.1364, 1.4300, 0.9051, 1.0197,
            1.7059, 0.8183, 1.4105, 1.2043, 0.8788, 0.8110, 0.9577, 0.9514, 0.7600,
            1.3303, 0.7729, 0.7576, 1.4356, 0.8852, 1.2144, 0.7648, 1.2414, 1.3303,
            1.3206, 1.0853, 0.7880, 1.2104, 0.8885, 0.7459, 0.7490, 1.0757, 1.1275,
            0.7855, 1.0083, 0.8174, 0.7584, 0.9851, 0.8641, 1.6821, 1.3996, 1.4736,
            0.8286, 1.0870, 0.9758, 0.7428, 1.0313, 0.7444, 1.1508, 0.9784, 1.3254,
            0.9745, 0.8799, 1.2543, 1.0615, 1.4979, 0.7812, 1.2653, 1.2063, 0.7656,
            1.0417, 1.0837, 1.0741, 1.5491, 0.8146, 1.2124, 1.4766, 0.8907, 1.3653,
            0.7821, 0.8600, 0.7812, 0.7880, 0.7608, 0.8128, 0.7746, 1.7262, 0.8831,
            1.6004, 1.7059, 1.0805, 0.9108, 1.2609, 1.0447, 1.0183, 1.2609, 0.7855,
            0.7474, 1.1490, 1.3040, 0.7729, 1.5795, 0.9062, 0.7584, 1.0298, 0.7584,
            0.7754, 1.1545, 1.2855, 1.1052, 1.1240, 0.9212, 1.3526, 0.7771, 0.8201,
            0.7863, 1.2675, 0.9452, 0.9142, 0.9904, 0.7941, 1.4471, 0.7863, 1.0387,
            0.8662, 0.7331, 1.7640, 1.0097, 0.7474, 1.0886, 0.9552, 1.3889, 1.2309,
            0.7721, 0.7746, 0.8885, 0.9877, 1.0112, 1.6590, 1.3862, 1.3134, 1.2832,
            1.3087, 1.2023, 1.0886, 0.8799, 0.8164, 0.8610, 0.8693, 0.7664, 0.7656,
            0.7680, 0.8411, 1.3327, 1.2063, 1.0432, 0.8767, 0.7967, 0.8430, 1.0342,
            1.1188, 0.8333, 1.1885, 1.4918, 1.3040, 1.3942, 0.7906, 0.8174, 0.8652,
            0.8391, 1.1018, 0.8276, 0.8767, 1.2104, 0.9331, 1.4948, 1.1035, 0.9236,
            0.8450, 0.8382, 0.8973, 1.2697, 1.0853, 1.3653, 0.8940, 0.9212, 1.0870,
            0.7889, 1.1846, 0.8201, 0.9811, 0.7353, 0.7497, 0.8621, 0.9539, 1.3278,
            0.8082, 1.0630, 0.8146, 1.0886, 1.0417, 1.1944, 0.9641, 1.0284, 0.7932,
            1.0328, 0.7688, 1.6183, 1.0902, 1.0328, 1.4826, 0.8305, 1.0902, 0.9006,
            0.8286, 1.0226, 0.7592, 0.7421, 0.8155, 1.0269, 0.7812, 1.2144, 0.9918,
            0.8831, 1.3451, 1.3810, 1.0821, 1.0069, 1.5591, 1.2923, 1.2478, 1.3602,
            1.4500, 1.7901, 0.8333, 0.8703, 0.8499, 1.3303, 1.4617, 1.3757, 0.7796,
            1.1750, 0.9718, 0.8324, 1.0183, 1.1885, 1.2675, 0.7950, 1.0935, 1.2675,
            0.7436, 1.2900, 1.0615, 0.8767, 1.1137, 0.8192, 0.7771, 1.0805, 0.9440,
            0.9391, 1.0677, 1.1453, 0.8918, 1.2809, 0.7713, 1.5041, 1.8262, 1.4414,
            0.8480, 1.3230, 0.7576, 0.7958, 1.2993, 1.1964, 1.1018, 0.7640, 1.2809,
            1.0837, 0.9932, 0.8211, 1.6477, 1.2764, 0.7552, 1.1137, 1.0741, 0.9074,
            0.7474, 0.7474, 1.2923, 1.5558, 0.9029, 1.2104, 1.2288, 1.5199, 1.2393,
            0.9166, 0.8984, 1.1453, 1.4676, 1.6628, 1.0773, 1.2787, 1.5199, 0.8257,
            1.4050, 1.0342, 1.0615, 0.8918, 0.7529, 0.7898, 1.1052, 0.7737, 0.7600,
            0.9452, 0.9877, 1.0055, 1.2970, 1.5263, 1.2522, 0.8610, 1.2587, 0.8220,
            0.9189, 1.0083, 0.8550, 0.8874, 1.3087, 0.7863, 1.3577, 0.7640, 1.2104,
            0.8038, 1.0553, 0.8641, 1.2522, 0.9224, 0.7737, 0.7544, 0.8092, 1.2330,
            1.0462, 1.2185, 1.7470, 0.9667, 1.1750, 0.9718, 0.9465, 2.0251, 1.1694,
            0.9603, 0.7608, 0.7584, 1.0837, 0.7787, 1.8080, 0.9680, 0.9119, 0.8863,
            0.9771, 0.9189, 1.7554, 1.6900, 1.0507, 0.7754, 1.2742, 1.6590, 1.2565,
            0.7490, 0.7976, 0.8201, 0.8011, 0.8305, 0.7863, 0.7713, 1.4272, 0.7608,
            1.6744, 1.3303, 0.9029, 1.3352, 1.0211, 1.1581, 1.0522, 1.8590, 1.7180,
            0.7421, 0.7368, 1.0789, 0.8056, 1.5795, 1.9079, 1.0183, 1.0805, 0.8831,
            1.3376, 0.7705, 0.8896, 0.8192, 1.2164, 0.9017, 1.2104, 1.1490, 0.9017,
            1.0328, 1.0709, 1.2742, 1.3996, 1.5360, 0.8841, 0.7521, 1.1206, 1.1453,
            1.0935, 0.8239, 0.7713, 0.9502, 0.7950, 1.0477, 0.9062, 0.7696, 0.9391,
            0.7872, 0.8820, 1.0919, 1.1808, 0.8229, 0.7552, 1.0538, 0.7656, 1.4558,
            1.2697, 0.7721, 0.8724, 0.7406, 0.7624, 1.8880, 0.8621, 1.1675, 1.4588,
            1.1188, 0.8343, 1.3996, 1.4796, 0.8211, 1.8262, 1.0492, 1.0630, 0.9680,
            1.1206, 0.8073, 0.9758, 0.7413, 0.9577, 0.9006, 1.2543, 0.7664, 1.0432,
            0.7958, 1.6040, 0.8333, 1.1293, 1.4918, 1.3810, 0.8874, 1.2288, 0.7521,
            1.4160, 0.9864, 0.8372, 0.8450, 0.8580, 1.3942, 1.2164, 0.9851, 0.9837,
            0.9166, 0.7737, 0.8248, 1.5899, 1.4050, 0.8907, 1.5458, 0.7721, 0.7737,
            1.1035, 0.7436, 0.7696, 0.9972, 0.8885, 1.0584, 1.1905, 1.0935, 1.0741,
            1.3278, 0.8529, 1.0402, 0.7721, 1.1154, 0.8550, 0.7608, 0.7529, 0.7608,
            0.8343, 0.8002, 1.6667, 0.8119, 1.1417, 0.7459, 1.1052, 1.4105, 1.3916,
            0.8220, 1.6782, 1.1924, 1.4857, 1.0837, 1.2877, 1.5073, 1.5136, 0.8440,
            1.1035, 0.7705, 0.7560, 1.5969, 0.7762, 0.7600, 1.4558, 0.8896, 1.1018,
            0.8550, 1.1346, 1.2697, 1.2809, 1.6075, 0.7640, 1.1171, 0.7705, 0.7898,
            1.2185, 1.2104, 1.1924, 0.8343, 1.4736, 1.0677, 0.7950, 0.7544, 0.8137,
            0.8056, 0.9784, 0.7967, 1.3206, 1.6667, 1.2247, 1.2970, 0.9615, 0.7746,
            0.8799, 1.0773, 0.9490, 1.0773, 1.0741, 0.9307, 1.2522, 0.9247, 1.3836,
            0.8693, 1.3426, 1.5830, 0.9331, 0.9440, 1.6219, 0.7490, 1.2500, 0.9040,
            0.8539, 2.0480, 0.9824, 0.7467, 0.8460, 0.8777, 0.8841, 1.6256, 1.1866,
            1.1944, 1.1619, 1.2267, 0.9403, 1.1964, 1.3158, 1.0477, 1.2083, 1.3134,
            1.3303, 1.0097, 0.8885, 0.8683, 1.6040, 0.7592, 0.9877, 1.1258, 1.2436,
            1.0313, 0.8248, 1.0154, 0.8128, 1.0357, 1.1750, 0.9236, 0.9877, 1.3016,
            0.7552, 1.0402, 0.8011, 0.8101, 0.7536, 1.2247, 1.7386, 1.3996, 0.9972,
            1.1545, 0.9189, 0.8353, 1.0000, 0.7616, 1.0853, 0.9784, 1.2565, 1.0000,
            0.7451, 0.8756, 0.9440, 0.9771, 1.2478, 0.9259, 0.7941
        ])
    else:
        print('there is no weight appropriate to the number of classes')
        print('class weight will not be applied to loss funciton')
        class_weight = None

    return class_weight
